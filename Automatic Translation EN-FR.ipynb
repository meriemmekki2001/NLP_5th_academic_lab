{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0c0d1d-80e8-4ea3-a6d3-f208f9d72820",
   "metadata": {},
   "source": [
    "### Automatic Translation FR to EN using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea0616d-283b-46b1-951c-9919f4d4d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 23:09:15.628367: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-19 23:09:15.707626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 23:09:18.292699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3df870-12a3-4865-b1ec-62567725aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dc2801-5a64-4707-89bb-be805b68b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wmt14_translate/fr-en'\n",
    "data_dir = 'nlp_lab_dataset/'\n",
    "train_samples = 60000  \n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset with specified splits\n",
    "ds_splits = tfds.load(dataset_name, split=['train', 'validation', 'test'], data_dir=data_dir)\n",
    "\n",
    "# Take a subset of the training set\n",
    "train_ds = ds_splits[0].take(train_samples)\n",
    "val_ds = ds_splits[1]\n",
    "test_ds = ds_splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec0f8e4-0185-4c9f-b258-417c612ceff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 23:09:31.364283: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-05-19 23:09:31.771480: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of validation samples: 3000\n",
      "Number of test samples: 3003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 23:09:32.476546: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Function to count number of samples in a dataset\n",
    "def count_samples(dataset):\n",
    "    return sum(1 for _ in dataset)\n",
    "\n",
    "# Count samples in each dataset\n",
    "num_train_samples = count_samples(train_ds)\n",
    "num_val_samples = count_samples(val_ds)\n",
    "num_test_samples = count_samples(test_ds)\n",
    "\n",
    "print(f\"Number of training samples: {num_train_samples}\")\n",
    "print(f\"Number of validation samples: {num_val_samples}\")\n",
    "print(f\"Number of test samples: {num_test_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7913c9-d46f-4162-a29b-64400a66dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 23:09:32.672636: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set samples:\n",
      "> Examples in English:\n",
      "In his briefing on economic development, Al Horner will give you details of programs we fund to foster partnerships between the private sector and First Nations and Inuit communities, in areas like resource development projects, for example.\n",
      "(b) Positive aspects\n",
      "Crop insurance payments include only government crop insurance programs; private hail insurance payments are excluded.\n",
      "\n",
      "> Examples in French:\n",
      "Dans sa présentation sur le développement économique, M. Al Horner vous donnera des détails sur les programmes que nous finançons pour favoriser l'établissement de partenariats entre le secteur privé et les collectivités des Premières nations et inuites dans des domaines comme celui de l'exploitation des ressources naturelles.\n",
      "b) Aspects positifs\n",
      "Les indemnités d’assurance-récolte comprennent uniquement celles des programmes publics; les indemnités de l’assurance-grêle privée sont exclues.\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the subsets\n",
    "print(\"Training set samples:\")\n",
    "for batch in train_ds.batch(3).take(1):\n",
    "    print('> Examples in English:')\n",
    "    en_examples = batch[\"en\"].numpy()\n",
    "    for en in en_examples:\n",
    "        print(en.decode(\"utf-8\"))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print('> Examples in French:')\n",
    "    fr_examples = batch[\"fr\"].numpy()\n",
    "    for fr in fr_examples:\n",
    "        print(fr.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b6543-a3b3-4588-8f3c-acfa7df0b013",
   "metadata": {},
   "source": [
    "#### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d233b3a-94a1-429b-9594-b0350cb16548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./fr_en_tokenizer.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name = 'fr_en_tokenizer'\n",
    "tf.keras.utils.get_file(\n",
    "    f'{tokenizer_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{tokenizer_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2e5fb6-5a0f-4627-af49-f7ba770b7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de03c84-19e6-4a47-a31e-f395ed24864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fec33e-48ed-49e3-9d4b-310230d8c67d",
   "metadata": {},
   "source": [
    "### Set up a data pipeline with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72001666-9c22-4e59-8943-cc62363a093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128\n",
    "def prepare_batch(batch):\n",
    "    fr = batch[\"fr\"]\n",
    "    en = batch[\"en\"]\n",
    "    fr = tokenizers.fr.tokenize(fr)      # Output is ragged.\n",
    "    fr = fr[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    fr = fr.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    en = en[:, :(MAX_TOKENS+1)]\n",
    "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (fr, en_inputs), en_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b82e367-6ba5-45fa-8bc5-798f3a93079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b58a27-378c-410a-b894-a68ff87ee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797fad89-d097-433d-b36f-f3bb5e8dd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(train_ds)\n",
    "val_batches = make_batches(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa2cac0-4541-4235-914b-40131ce11e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 23:09:35.903973: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "(64, 128)\n",
      "(64, 128)\n"
     ]
    }
   ],
   "source": [
    "for (fr, en), en_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(fr.shape)\n",
    "print(en.shape)\n",
    "print(en_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce7f939-7980-46b3-85b6-87443c8c8b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   2  295  434  292 1109   15  350  306 1097  346], shape=(10,), dtype=int64)\n",
      "tf.Tensor([ 295  434  292 1109   15  350  306 1097  346   17], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(en[0][:10])\n",
    "print(en_labels[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c39d6b-75e9-4d8d-abe6-df4d6ddf6847",
   "metadata": {},
   "source": [
    "### Define the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7767199-180f-413f-81e8-e102782f9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91cdeda-cae9-49a9-9d1f-1eff97f8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e260c55e-3125-4e9c-bbc7-a733f132f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_fr = PositionalEmbedding(vocab_size=tokenizers.fr.get_vocab_size().numpy(), d_model=512)\n",
    "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size().numpy(), d_model=512)\n",
    "\n",
    "fr_emb = embed_fr(fr)\n",
    "en_emb = embed_en(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4830f2a-a726-4ad6-8081-e566748e6a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 128), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25400037-faa5-4cad-84da-e7407e16c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c122ee59-1be0-4252-ab97-5c778a3097e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
    "result = d['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e628886-5d33-4e20-b2c1-455fff467456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5c4f1a-d895-40b7-b92f-b33c19827b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'cross_attention' (of type CrossAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(fr_emb.shape)\n",
    "print(en_emb.shape)\n",
    "print(sample_ca(en_emb, fr_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "119b0f12-e1da-4399-a360-af40d0e189d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f042403-4b55-451d-8898-0ad3e5e566db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(fr_emb.shape)\n",
    "print(sample_gsa(fr_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648b2743-3ca2-475f-9cd0-c7524be3529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29fb8d2b-0b3d-4e37-a3c3-02340ae952ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_csa(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35e8871b-80c4-4555-b117-99f10d899550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7683716e-07"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = sample_csa(embed_en(en[:, :3])) \n",
    "out2 = sample_csa(embed_en(en))[:, :3]\n",
    "\n",
    "tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73c2a839-ffab-4b8e-809f-b68e30ade55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e9eedf3-7cd6-4292-876f-3179f65cfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'sequential' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'feed_forward' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_ffn = FeedForward(512, 2048)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_ffn(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60bdbe84-a901-46f0-94df-dc1af982e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d1d4b44-fca3-464b-b856-5bdb1faf729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'global_self_attention_1' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "print(fr_emb.shape)\n",
    "print(sample_encoder_layer(fr_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91fb868c-e8a3-4854-87b9-68dd78338c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5f67642-3b83-4f05-8b3b-cb9ae40a4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'global_self_attention_2' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'encoder_layer_1' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "(64, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(fr, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(fr.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1385da89-8f86-4579-9eed-62ede61cbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bc776d4-5297-47c2-bbfc-62492d424c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'causal_self_attention_1' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 512)\n",
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    x=en_emb, context=fr_emb)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(fr_emb.shape)\n",
    "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62feae42-d624-4081-962f-68d6a80fb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9ccd662-ce22-4c4d-9f50-d2044a65c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'causal_self_attention_2' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'decoder_layer_1' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "(64, 128, 512)\n",
      "(64, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=en,\n",
    "    context=fr_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(en.shape)\n",
    "print(fr_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88fb206a-dc41-4d05-be02-12c74d071915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 8, 128, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b143d1f-9294-4e79-a830-30196a0cec70",
   "metadata": {},
   "source": [
    "### The Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fdc9990-c53b-44f8-8d63-b8ee8ab4f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25b97e-478d-4712-8b72-d3d2d8d7ee99",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97813055-1846-49db-bb16-99dfb4323253",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c1d521f-6d3e-44a5-b716-fcbaa9b2c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ebf5052-cdef-4cdf-b9a2-e4bb1408e639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'global_self_attention_6' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'encoder_layer_5' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'causal_self_attention_6' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/meriem-mk/.local/lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'decoder_layer_5' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n",
      "(64, 128)\n",
      "(64, 128, 7955)\n"
     ]
    }
   ],
   "source": [
    "output = transformer((fr, en))\n",
    "\n",
    "print(en.shape)\n",
    "print(fr.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22c5f521-c751-469a-a1de-7a31bc3b8331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 8, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be34fd71-e0c4-41eb-8902-5d1f6fb0a7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,620,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,768,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026,195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m3,620,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m5,768,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │     \u001b[38;5;34m1,026,195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,414,739</span> (39.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,414,739\u001b[0m (39.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,414,739</span> (39.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,414,739\u001b[0m (39.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcec15-db4e-4639-9c64-ec835d68814d",
   "metadata": {},
   "source": [
    "#### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "302205b7-5c19-4edb-a695-4da23ecebc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44822052-1ad7-4141-ace1-65425c7db265",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f0d9a2-c89b-4019-b1a1-2a314d7b0df9",
   "metadata": {},
   "source": [
    "#### Set up the loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a507e0a1-571b-4d6d-aee9-d54e4495a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45513640-2e34-4608-b791-24ae6a9371dd",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb571dad-14f2-4c12-a7f7-2d0e9fde5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "887d9f2f-1fc3-4c5e-b8f2-329eae7cba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4166s\u001b[0m 4s/step - loss: 7.9205 - masked_accuracy: 0.0713 - val_loss: 5.8780 - val_masked_accuracy: 0.1724\n",
      "Epoch 2/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4104s\u001b[0m 4s/step - loss: 5.4704 - masked_accuracy: 0.2091 - val_loss: 5.0699 - val_masked_accuracy: 0.2249\n",
      "Epoch 3/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4041s\u001b[0m 4s/step - loss: 4.6238 - masked_accuracy: 0.2747 - val_loss: 4.5022 - val_masked_accuracy: 0.2751\n",
      "Epoch 4/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4061s\u001b[0m 4s/step - loss: 4.0105 - masked_accuracy: 0.3336 - val_loss: 4.1594 - val_masked_accuracy: 0.3144\n",
      "Epoch 5/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4061s\u001b[0m 4s/step - loss: 3.5639 - masked_accuracy: 0.3830 - val_loss: 3.8103 - val_masked_accuracy: 0.3547\n",
      "Epoch 6/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3971s\u001b[0m 4s/step - loss: 3.1521 - masked_accuracy: 0.4344 - val_loss: 3.5802 - val_masked_accuracy: 0.3857\n",
      "Epoch 7/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2928s\u001b[0m 3s/step - loss: 2.8566 - masked_accuracy: 0.4722 - val_loss: 3.4738 - val_masked_accuracy: 0.4003\n",
      "Epoch 8/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2741s\u001b[0m 3s/step - loss: 2.6398 - masked_accuracy: 0.5025 - val_loss: 3.3779 - val_masked_accuracy: 0.4176\n",
      "Epoch 9/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4950s\u001b[0m 5s/step - loss: 2.4570 - masked_accuracy: 0.5289 - val_loss: 3.3160 - val_masked_accuracy: 0.4272\n",
      "Epoch 10/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4366s\u001b[0m 5s/step - loss: 2.3105 - masked_accuracy: 0.5503 - val_loss: 3.3170 - val_masked_accuracy: 0.4308\n",
      "Epoch 11/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3911s\u001b[0m 4s/step - loss: 2.1858 - masked_accuracy: 0.5688 - val_loss: 3.3095 - val_masked_accuracy: 0.4354\n",
      "Epoch 12/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5473s\u001b[0m 6s/step - loss: 2.0756 - masked_accuracy: 0.5849 - val_loss: 3.3369 - val_masked_accuracy: 0.4358\n",
      "Epoch 13/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5199s\u001b[0m 6s/step - loss: 1.9810 - masked_accuracy: 0.5991 - val_loss: 3.3836 - val_masked_accuracy: 0.4331\n",
      "Epoch 14/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5192s\u001b[0m 6s/step - loss: 1.8977 - masked_accuracy: 0.6119 - val_loss: 3.4329 - val_masked_accuracy: 0.4342\n",
      "Epoch 15/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4147s\u001b[0m 4s/step - loss: 1.8055 - masked_accuracy: 0.6263 - val_loss: 3.4946 - val_masked_accuracy: 0.4298\n",
      "Epoch 16/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5295s\u001b[0m 6s/step - loss: 1.7301 - masked_accuracy: 0.6384 - val_loss: 3.5351 - val_masked_accuracy: 0.4276\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 18:14:19.293803: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:299: Filling up shuffle buffer (this may take a while): 1 of 20000\n",
      "2024-05-20 18:14:20.076203: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9151s\u001b[0m 10s/step - loss: 1.6550 - masked_accuracy: 0.6501 - val_loss: 3.5856 - val_masked_accuracy: 0.4236\n",
      "Epoch 18/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5407s\u001b[0m 6s/step - loss: 1.5895 - masked_accuracy: 0.6613 - val_loss: 3.6507 - val_masked_accuracy: 0.4261\n",
      "Epoch 19/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5337s\u001b[0m 6s/step - loss: 1.5143 - masked_accuracy: 0.6738 - val_loss: 3.7104 - val_masked_accuracy: 0.4204\n",
      "Epoch 20/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5374s\u001b[0m 6s/step - loss: 1.4578 - masked_accuracy: 0.6830 - val_loss: 3.7793 - val_masked_accuracy: 0.4176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f384212dc10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e17368f-bef1-4550-9a0e-b0fb8f16ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.fr.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1600c8d7-2558-4798-9b82-7637324bb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fefc4028-12de-45e1-a877-269f6901cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c272361-b395-4b7c-8aaa-a134028b7663",
   "metadata": {},
   "source": [
    "### The model predicted accurately these two exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fed60cdf-1a19-4111-9f62-cbcf1d235a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Dans sa présentation sur le développement économique, M. Al Horner vous donnera des détails sur les programmes que nous finançons pour favoriser l'établissement de partenariats entre le secteur privé et les collectivités des Premières nations et inuites dans des domaines comme celui de l'exploitation des ressources naturelles.\n",
      "Prediction     : in presentation on economic development , al . m h . al horner will give you details of programmes which we funded with partnerships between the private sector and first nation communities and inuit in development in areas such as exploitation of natural resources .\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Dans sa présentation sur le développement économique, M. Al Horner vous donnera des détails sur les programmes que nous finançons pour favoriser l'établissement de partenariats entre le secteur privé et les collectivités des Premières nations et inuites dans des domaines comme celui de l'exploitation des ressources naturelles.\"\n",
    "ground_truth = ''\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "385df922-3a62-4265-98d2-c8d291e20a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         :  Aspects positifs Les indemnités d’assurance-récolte comprennent uniquement celles des programmes publics; les indemnités de l’assurance-grêle privée sont exclues.\n",
      "Prediction     : positive aspects of - harvest compensation are only those in public programs ; crown - crown compensation are excluded .\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence = \" Aspects positifs Les indemnités d’assurance-récolte comprennent uniquement celles des programmes publics; les indemnités de l’assurance-grêle privée sont exclues.\"\n",
    "ground_truth = ''\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a34d0-bcf8-4ab0-b74d-f146616ac0e4",
   "metadata": {},
   "source": [
    "### The model failed in predicting these other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9b46c6d-af37-4d25-a574-547694620fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Salut, comment Ça va?\n",
      "Prediction     : known how many , how or not , how orly , how , how or because of how dos not stays only ? but what of how to stay , how , in the point of how , how , how , how , how , how , something , in the night , how , to stay , for times , when that point to point of how to stays are going to stay that ? not ? not , to stay , to point of how , how , of how , one stay , one point of how ? only only only , do , not , not , not , not , only , not , only , not ,\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Salut, comment Ça va?'\n",
    "ground_truth = ''\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f01b0a37-9afb-43b5-835e-0f03158cd8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Bonjour.\n",
      "Prediction     : good faith in the media with media with natural background .\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Bonjour.'\n",
    "ground_truth = ''\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83217a2d-4d52-49a5-b91c-de626badb86f",
   "metadata": {},
   "source": [
    "### Export the model for later usage (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55e9df8a-b98b-44ee-85b1-8dead8ba5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result,\n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "822d11b7-0842-4b7f-9261-5db72bb7cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f07060db-7c84-43cd-95ed-ec19e83673a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the test is not applicable in the test .'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('test.').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8a0d9ad-3767-4597-9bcf-7fb74d190616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator-fr-en/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator-fr-en/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator-fr-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8262b-862d-413a-a21d-7535dd306e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
